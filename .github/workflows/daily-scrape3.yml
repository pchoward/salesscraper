name: Daily Scrape Job

on:
  push:
    branches:
      - main
  schedule:
    - cron: '0 0 * * *' # Run daily at midnight UTC
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      # Check out the repository
      - name: Checkout code
        uses: actions/checkout@v4

      # Set up Python
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9' # Match Python 3.9.22

      # Install Chrome and dependencies
      - name: Install Chrome and dependencies
        run: |
          sudo apt-get update
          # Install dependencies for Chrome and Xvfb, including x11-utils for xdpyinfo
          sudo apt-get install -y xvfb libx11-xcb1 libdbus-glib-1-2 libxt6 libgtk-3-0 libasound2t64 x11-utils
          # Add Google Chrome repository
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
          sudo apt-get update
          # Install the latest stable version of Chrome
          sudo apt-get install -y google-chrome-stable
          # Get the installed Chrome version
          CHROME_VERSION=$(google-chrome --version | grep -oP '\d+\.\d+\.\d+\.\d+')
          echo "Installed Chrome version: $CHROME_VERSION"

      # Install chromedriver
      - name: Install chromedriver
        run: |
          # Fetch the matching chromedriver version for the installed Chrome version
          CHROME_VERSION=$(google-chrome --version | grep -oP '\d+\.\d+\.\d+\.\d+')
          wget https://storage.googleapis.com/chrome-for-testing-public/$CHROME_VERSION/linux64/chromedriver-linux64.zip
          unzip chromedriver-linux64.zip
          sudo mv chromedriver-linux64/chromedriver /usr/local/bin/
          sudo chmod +x /usr/local/bin/chromedriver
          chromedriver --version

      # Install Python dependencies
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Run the scraper with Xvfb
      - name: Run scraper
        run: |
          Xvfb :99 -screen 0 1920x1080x24 -ac -noreset &
          # Add a small delay to ensure Xvfb is fully initialized
          sleep 2
          export DISPLAY=:99
          # Verify Xvfb is running
          xdpyinfo -display :99 || echo "Xvfb failed to start"
          python zumiez_analyzer-grok2.py
        env:
          PYTHONUNBUFFERED: 1

      # Commit and push the updated HTML file
      - name: Commit and push changes
        run: |
          git config --global user.name "GitHub Actions Bot"
          git config --global user.email "actions@github.com"
          git add sale_items_chart.html
          git commit -m "Update sale_items_chart.html with latest scrape data" || echo "No changes to commit"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      # Upload artifacts
      - name: Upload artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-output
          path: |
            *.html
            *.json